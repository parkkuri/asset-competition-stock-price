{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import lxml\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#증권뉴스 크롤링\n",
    "#-*- coding:utf-8 -*-\n",
    "\n",
    "page_num = 14\n",
    "max_page_num = 1000\n",
    "\n",
    "user_agent = \"'Mozilla/5.0\"\n",
    "headers ={\"User-Agent\" : user_agent}\n",
    "\n",
    "urls_final = []\n",
    "\n",
    "while page_num<=max_page_num:\n",
    "\n",
    "\n",
    "    page_url = \"https://finance.naver.com/news/market_special.nhn?&page=\"+ str(page_num) + \"\"\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    html = response.text\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    주어진 HTML에서 기사 URL을 추출한다.\n",
    "    \"\"\"\n",
    "    urls_final2=[]\n",
    "    urls=[]\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    url_frags = soup.select(\"td.publicSubject > a\")\n",
    "    href2=[]\n",
    "    \n",
    "    for i in url_frags:\n",
    "        href = i.attrs['href']\n",
    "        href2.append(href)\n",
    "        \n",
    "    for a in href2:\n",
    "        aa = \"https://finance.naver.com\" + str(a) + \"\"\n",
    "        urls_final.append(aa)\n",
    "    \n",
    "    urls_final2.extend(urls_final)    \n",
    "    \n",
    "    page_num+=1\n",
    "    \n",
    "    rank1=[]\n",
    "title1 =[]\n",
    "data1=[]\n",
    "url_final3=[]\n",
    "news=[]\n",
    "for URLs in urls_final2:\n",
    "    s = requests.get(URLs)\n",
    "    plain_text = s.text\n",
    "    soup = BeautifulSoup(plain_text, \"html.parser\")\n",
    "    content2 = soup.select_one(\"div.articleCont\").text\n",
    "    content3=str(content2)\n",
    "    content4=content3.split('<a href=')[0]\n",
    "    content4 = content4.replace('<div class=\"articleCont\" id=\"content\">','')\n",
    "    content4 = content4.replace('<br/>','')\n",
    "    content4 = content4.replace('\\n\\t\\t\\t\\t\\t\\t','')\n",
    "    content5 =  content4.split('▶')[0]\n",
    "    content5 = content5.split('@')[0]\n",
    "    title = soup.select_one(\"li.nowArticle\")\n",
    "    title = title.text\n",
    "    data = soup.select_one(\"span.article_date\").text\n",
    "    url_s = URLs\n",
    "    url_final3.append(url_s)\n",
    "    title1.append(title)\n",
    "    data1.append(data)\n",
    "    news.append(content5)\n",
    "    \n",
    "\n",
    "data= [title1, data1, news,url_final3]\n",
    "\n",
    "newData=np.transpose(data)\n",
    "\n",
    "df = pd.DataFrame(newData, columns=['title1', 'date', 'news','urls'])\n",
    "\n",
    "df.to_csv('C:/Users/kebee/Desktop/python/new_data1.csv',encoding=\"UTF-16\",mode=\"w\",index=True, sep='\\t')\n",
    "\n",
    "data= [title1, data1, news,url_final3]\n",
    "\n",
    "newData=np.transpose(data)\n",
    "\n",
    "df = pd.DataFrame(newData, columns=['title1', 'date', 'news','urls'])\n",
    "\n",
    "df.to_csv('C:/Users/kebee/Desktop/python/new_data1.csv',encoding=\"UTF-16\",mode=\"w\",index=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "\n",
    "page_num = 1001\n",
    "max_page_num = 4000\n",
    "\n",
    "user_agent = \"'Mozilla/5.0\"\n",
    "headers ={\"User-Agent\" : user_agent}\n",
    "\n",
    "urls_final = []\n",
    "\n",
    "while page_num<=max_page_num:\n",
    "\n",
    "\n",
    "    page_url = \"https://finance.naver.com/news/market_special.nhn?&page=\"+ str(page_num) + \"\"\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    html = response.text\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    주어진 HTML에서 기사 URL을 추출한다.\n",
    "    \"\"\"\n",
    "    urls_final2=[]\n",
    "    urls=[]\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    url_frags = soup.select(\"td.publicSubject > a\")\n",
    "    href2=[]\n",
    "    \n",
    "    for i in url_frags:\n",
    "        href = i.attrs['href']\n",
    "        href2.append(href)\n",
    "        \n",
    "    for a in href2:\n",
    "        aa = \"https://finance.naver.com\" + str(a) + \"\"\n",
    "        urls_final.append(aa)\n",
    "    \n",
    "    urls_final2.extend(urls_final)    \n",
    "    \n",
    "    page_num+=1\n",
    "    \n",
    "    rank1=[]\n",
    "    \n",
    "title1 =[]\n",
    "data1=[]\n",
    "url_final3=[]\n",
    "news=[]\n",
    "count=0\n",
    "for URLs in urls_final2:\n",
    "    s = requests.get(URLs)\n",
    "    ss=s.text\n",
    "    sss = '언론사에 의해 삭제되었거나, 존재하지 않는 기사입니다.'\n",
    "    if ss.find(sss)!=-1:\n",
    "        next\n",
    "    else:\n",
    "        plain_text = s.text\n",
    "        soup = BeautifulSoup(plain_text, \"html.parser\")\n",
    "        content2 = soup.select_one(\"div.articleCont\").text\n",
    "        content3=str(content2)\n",
    "        content4=content3.split('<a href=')[0]\n",
    "        content4 = content4.replace('<div class=\"articleCont\" id=\"content\">','')\n",
    "        content4 = content4.replace('<br/>','')\n",
    "        content4 = content4.replace('\\n\\t\\t\\t\\t\\t\\t','')\n",
    "        content5 =  content4.split('▶')[0]\n",
    "        content5 = content5.split('@')[0]\n",
    "        title = soup.select_one(\"li.nowArticle\")\n",
    "        title = title.text\n",
    "        data = soup.select_one(\"span.article_date\").text\n",
    "        url_s = URLs\n",
    "        url_final3.append(url_s)\n",
    "        title1.append(title)\n",
    "        data1.append(data)\n",
    "        news.append(content5)\n",
    "    count+=1\n",
    "    print(url_s)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title11 = DataFrame(title1)\n",
    "data11 = DataFrame(data1)\n",
    "\n",
    "news11 = DataFrame(news)\n",
    "\n",
    "url_final11 = DataFrame(url_final3)\n",
    "\n",
    "data111= pd.concat([title11,data11, news11,url_final11],axis=1)\n",
    "df = pd.DataFrame(data111)\n",
    "\n",
    "df.columns=['title1', 'date', 'news','urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/kebee/Desktop/python/new_data_add_big.csv',encoding=\"UTF-16\",mode=\"w\",index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##재무제표 크롤링\n",
    "\n",
    "import requests as reqq\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request as req\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import lxml\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:/Users/kebee/Desktop/python/stock_code.csv\", 'r')\n",
    "lines = f.readlines()\n",
    "company_code = []\n",
    "for line in lines[1:]:\n",
    "    line= line.replace('\\n','')\n",
    "    company_code.append(line)\n",
    "f.close()\n",
    "print(company_code)\n",
    "\n",
    "#-*- coding:utf-8 -*-\n",
    "df_final_real2 = DataFrame([])\n",
    "count = 1 \n",
    "for code in company_code:\n",
    "    if company_code=='NA':\n",
    "        continue\n",
    "    code2 = (6 - len(code))  * '0' + code\n",
    "    url = 'http://companyinfo.stock.naver.com/v1/company/ajax/cF1001.aspx?cmp_cd=%s&fin_typ=0&freq_typ=Y'%(code2)\n",
    "    dfs = pd.read_html(url)\n",
    "    df = dfs[0]\n",
    "    df2 = df.iloc[:,1:6]\n",
    "    try:\n",
    "        df2.columns = ['2013_12','2014_12','2015_12','2016_12','2017_12']\n",
    "    except ValueError:\n",
    "        continue\n",
    "    df_index=DataFrame(['매출액','영업이익','영업이익(발표기준)','세전계속사업이익','당기순이익','  당기순이익(지배)','  당기순이익(비지배)','자산총계','부채총계','자본총계','  자본총계(지배)','자본총계(비지배)','자본금','영업활동현금흐름','투자활동현금흐름','재무활동현금흐름','CAPEX','FCF','이자발생부채','영업이익률','순이익률','ROE(%)','ROA(%)','부채비율','자본유보율','EPS(원)','PER(배)','BPS(원)','PBR(배)','현금DPS(원)','현금배당수익률','현금배당성향(%)','발행주식수(보통주)'],columns = ['index1'])\n",
    "    code2 = code*32\n",
    "    df_final=pd.concat([df_index,df2],axis=1)\n",
    "    Code1 = DataFrame([code]*32,columns = ['code'])\n",
    "    df_final_real=pd.concat([Code1,df_final],axis=1)\n",
    "    df_final_real2 = df_final_real2.append(df_final_real)\n",
    "    print(count)\n",
    "    print(code)\n",
    "    count = count + 1\n",
    "    \n",
    "df_final_real2.to_csv(\"C:/Users/kebee/Desktop/python/company_data_real_new.csv\",encoding=\"euc-kr\",mode=\"w\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as reqq\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request as req\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import lxml\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#설립년도 크롤링\n",
    "f = open(\"C:/Users/kebee/Desktop/python/stock_code.csv\", 'r')\n",
    "lines = f.readlines()\n",
    "company_code = []\n",
    "for line in lines[1:]:\n",
    "    line= line.replace('\\n','')\n",
    "    company_code.append(line)\n",
    "f.close()\n",
    "print(company_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_all = []\n",
    "\n",
    "for code in company_code :\n",
    "    if code=='NA':\n",
    "        continue\n",
    "    code2 = (6 - len(code))  * '0' + code\n",
    "    url = 'http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=%s&nav=1'% (code2)\n",
    "    url2 = reqq.get(url)\n",
    "    url2 = url2.content\n",
    "    html = BeautifulSoup(url2,'html.parser')\n",
    "\n",
    "    html2= str(html)\n",
    "    text = html.text\n",
    "    pattern = r'<td>\\d\\d\\d\\d[.]\\d\\d[.]\\d\\d'\n",
    "\n",
    "    try:\n",
    "        match1 = re.findall(pattern,html2)[0]\n",
    "    except IndexError:\n",
    "        continue\n",
    "    match1 = match1.replace('<td>','')\n",
    "    \n",
    "    data_all.append([code, match1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"code_compnay\",\"buliding\"]\n",
    "df = DataFrame(data_all, columns = columns)\n",
    "df.to_csv(\"C:/Users/kebee/Desktop/python/company_buliding.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
